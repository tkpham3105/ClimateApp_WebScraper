{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "burning-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "synthetic-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of Earth.org that need to be scraped \n",
    "INIT_URL = 'https://earth.org/climate-change/'\n",
    "\n",
    "# Prevent URL from detecting scraper as a bot when requesting \n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.148 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alleged-strain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Processing sub_page number 1 ======================\n",
      "===================== Processing sub_page number 2 ======================\n",
      "===================== Processing sub_page number 3 ======================\n",
      "===================== Processing sub_page number 4 ======================\n",
      "===================== Processing sub_page number 5 ======================\n",
      "===================== Processing sub_page number 6 ======================\n",
      "===================== Processing sub_page number 7 ======================\n",
      "===================== Processing sub_page number 8 ======================\n",
      "===================== Processing sub_page number 9 ======================\n",
      "===================== Processing sub_page number 10 ======================\n",
      "===================== Processing sub_page number 11 ======================\n",
      "===================== Processing sub_page number 12 ======================\n",
      "===================== Processing sub_page number 13 ======================\n",
      "===================== Processing sub_page number 14 ======================\n",
      "===================== Processing sub_page number 15 ======================\n",
      "===================== Processing sub_page number 16 ======================\n",
      "===================== Processing sub_page number 17 ======================\n",
      "===================== Processing sub_page number 18 ======================\n",
      "===================== Processing sub_page number 19 ======================\n",
      "===================== Processing sub_page number 20 ======================\n",
      "===================== Processing sub_page number 21 ======================\n",
      "===================== Processing sub_page number 22 ======================\n",
      "===================== Processing sub_page number 23 ======================\n",
      "===================== Processing sub_page number 24 ======================\n",
      "===================== Processing sub_page number 25 ======================\n",
      "===================== Processing sub_page number 26 ======================\n",
      "===================== Processing sub_page number 27 ======================\n",
      "===================== Processing sub_page number 28 ======================\n",
      "===================== Processing sub_page number 29 ======================\n",
      "===================== Processing sub_page number 30 ======================\n",
      "===================== Processing sub_page number 31 ======================\n",
      "===================== Processing sub_page number 32 ======================\n",
      "===================== Processing sub_page number 33 ======================\n",
      "===================== Processing sub_page number 34 ======================\n",
      "===================== Processing sub_page number 35 ======================\n",
      "===================== Processing sub_page number 36 ======================\n",
      "===================== Processing sub_page number 37 ======================\n",
      "===================== Processing sub_page number 38 ======================\n",
      "===================== Processing sub_page number 39 ======================\n",
      "===================== Processing sub_page number 40 ======================\n",
      "===================== Processing sub_page number 41 ======================\n",
      "===================== Processing sub_page number 42 ======================\n",
      "===================== Processing sub_page number 43 ======================\n",
      "===================== Processing sub_page number 44 ======================\n",
      "===================== Processing sub_page number 45 ======================\n",
      "===================== Processing sub_page number 46 ======================\n",
      "===================== Processing sub_page number 47 ======================\n",
      "===================== Processing sub_page number 48 ======================\n",
      "===================== Processing sub_page number 49 ======================\n",
      "===================== Processing sub_page number 50 ======================\n",
      "===================== Processing sub_page number 51 ======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:37<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Earth.org contains many articles that are divided into many sub_pages\n",
    "sub_URL_track = 0 # to track each sub_pages\n",
    "\n",
    "# Create dataframe to hold scraped data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through each sub_page\n",
    "while(True):\n",
    "    \n",
    "    # Tracking sub_page\n",
    "    sub_URL_track += 1\n",
    "    \n",
    "    print(\"===================== Processing sub_page number {} ======================\".format(sub_URL_track))\n",
    "    \n",
    "    # Each sub_page differs from others by extra element in its URL: \"sf_paged\"\n",
    "    s_page = requests.get('{}?sf_paged={}'.format(INIT_URL, sub_URL_track), headers = HEADERS) # remember to pass the defined HEADERS\n",
    "    soup = BeautifulSoup(s_page.content, \"html.parser\")\n",
    "    \n",
    "    # Each sub_page contains many articles with their unique URL\n",
    "    try:\n",
    "        # Find all tags containing articles within this sub_page\n",
    "        df['Page {}'.format(sub_URL_track)] = soup.find_all('article')\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# Main part: scrape article using its corresponding URL \n",
    "def scrape_article(URL):\n",
    "    \n",
    "    # Scraping request \n",
    "    a_page = requests.get(URL, headers = HEADERS)\n",
    "    soup = BeautifulSoup(a_page.content, \"html.parser\")\n",
    "    \n",
    "    # Keep necessary information only\n",
    "    title, image, content = map(lambda x: soup.find('section', class_=x), \n",
    "                                     ['post-hero','post_image','post-content'])\n",
    "\n",
    "    return {'title'  : '{}'.format(title),\n",
    "            'content': '{}{}'.format(image, content),\n",
    "            'URL'    : URL}        \n",
    "\n",
    "# tqdm helps observe progress of applying function on dataframe \n",
    "tqdm.pandas()\n",
    "\n",
    "# Map each cell in dataframe to its corresponding article URL then perform scraping for each URL accordingly\n",
    "df = df.progress_applymap(lambda x: scrape_article(x.find('a')['href']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
